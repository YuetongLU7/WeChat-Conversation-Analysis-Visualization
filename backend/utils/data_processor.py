"""
Data processing module for WeChat chat records
"""
import os
import pandas as pd
import jieba
import re
from collections import Counter
from datetime import datetime
import matplotlib
# Use Agg backend to avoid thread issues
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.dates as mdates
import numpy as np
from pyecharts import options as opts
from pyecharts.charts import WordCloud as PyechartsWordCloud
from pyecharts.globals import ThemeType
from googletrans import Translator

# æ·»åŠ é…ç½®å˜é‡ - æ˜¯å¦å¯ç”¨emojiç¬¦å·
USE_EMOJI_SYMBOLS = True

# Create translator instance
translator = Translator()

# æ–°å¢emojiç¬¦å·æ˜ å°„å­—å…¸
EMOJI_TO_SYMBOL = {
    # åŸºæœ¬è¡¨æƒ…
    'å¾®ç¬‘': 'ğŸ˜Š', 'å¼€å¿ƒ': 'ğŸ˜„', 'å¤§ç¬‘': 'ğŸ˜‚', 'å¯çˆ±': 'ğŸ¥°',
    'æ„¤æ€’': 'ğŸ˜¡', 'æ‚²ä¼¤': 'ğŸ˜¢', 'æƒŠè®¶': 'ğŸ˜®', 'å®³æ€•': 'ğŸ˜¨',
    'æ— è¯­': 'ğŸ˜‘', 'å°´å°¬': 'ğŸ˜…', 'æ€è€ƒ': 'ğŸ¤”', 'ç‚¹èµ': 'ğŸ‘',
    'çˆ±å¿ƒ': 'â¤ï¸', 'ç«ç‘°': 'ğŸŒ¹', 'è›‹ç³•': 'ğŸ‚', 'ç¤¼ç‰©': 'ğŸ',
    'ç¬‘å“­': 'ğŸ˜‚', 'å®³ç¾': 'ğŸ˜Š', 'å§”å±ˆ': 'ğŸ¥º', 'å‘†': 'ğŸ˜¶',
    'ç–‘é—®': 'â“', 'æ™•': 'ğŸ˜µ', 'è¡°': 'ğŸ˜©', 'å¹æ°”': 'ğŸ˜®â€ğŸ’¨',
    
    # å¾®ä¿¡å¸¸ç”¨è¡¨æƒ…
    'å¼º': 'ğŸ’ª', 'å¼±': 'ğŸ‘', 'ok': 'ğŸ‘Œ', 'æ‹œæ‰˜': 'ğŸ™',
    'å‘²ç‰™': 'ğŸ˜', 'å·ç¬‘': 'ğŸ˜', 'å†è§': 'ğŸ‘‹', 'æŠ“ç‹‚': 'ğŸ˜«',
    'é¼“æŒ': 'ğŸ‘', 'æµæ±—': 'ğŸ˜“', 'æ†¨ç¬‘': 'ğŸ˜„', 'æ‚ é—²': 'ğŸ˜Œ',
    'å¥‹æ–—': 'ğŸ’ª', 'å’’éª‚': 'ğŸ¤¬', 'ç–‘é—®': 'â“', 'å˜˜': 'ğŸ¤«',
    'æ™•': 'ğŸ˜µ', 'æŠ˜ç£¨': 'ğŸ˜–', 'è¡°': 'ğŸ˜©', 'éª·é«…': 'ğŸ’€',
    'æ•²æ‰“': 'ğŸ‘Š', 'å†è§': 'ğŸ‘‹', 'é—­å˜´': 'ğŸ¤', 'é„™è§†': 'ğŸ˜¤',
    'é—ªç”µ': 'âš¡', 'å‘æŠ–': 'ğŸ˜¨', 'éš¾è¿‡': 'ğŸ˜', 'é…·': 'ğŸ˜',
    'æŠ é¼»': 'ğŸ‘ƒ', 'åç¬‘': 'ğŸ˜', 'å·¦å“¼å“¼': 'ğŸ˜¤', 'å³å“¼å“¼': 'ğŸ˜¤',
    'å“ˆæ¬ ': 'ğŸ¥±', 'é„™è§†': 'ğŸ˜¤', 'å§”å±ˆ': 'ğŸ¥º', 'å¿«å“­äº†': 'ğŸ˜¢',
    'é˜´é™©': 'ğŸ˜ˆ', 'äº²äº²': 'ğŸ˜˜', 'å“': 'ğŸ˜±', 'å¯æ€œ': 'ğŸ¥º',
    'èœåˆ€': 'ğŸ”ª', 'è¥¿ç“œ': 'ğŸ‰', 'å•¤é…’': 'ğŸº', 'ç¯®çƒ': 'ğŸ€',
    'ä¹’ä¹“': 'ğŸ“', 'æ‹¥æŠ±': 'ğŸ¤—', 'æ¡æ‰‹': 'ğŸ¤'
}

# æ–°å¢emojiçš„è‹±æ–‡ç¿»è¯‘
EMOJI_TO_ENGLISH = {
    # åŸºæœ¬è¡¨æƒ…
    'å¾®ç¬‘': 'Smile', 'å¼€å¿ƒ': 'Happy', 'å¤§ç¬‘': 'Laugh', 'å¯çˆ±': 'Cute',
    'æ„¤æ€’': 'Angry', 'æ‚²ä¼¤': 'Sad', 'æƒŠè®¶': 'Surprised', 'å®³æ€•': 'Scared',
    'æ— è¯­': 'Speechless', 'å°´å°¬': 'Embarrassed', 'æ€è€ƒ': 'Thinking', 'ç‚¹èµ': 'Thumbs up',
    'çˆ±å¿ƒ': 'Heart', 'ç«ç‘°': 'Rose', 'è›‹ç³•': 'Cake', 'ç¤¼ç‰©': 'Gift',
    'ç¬‘å“­': 'Laugh Cry', 'å®³ç¾': 'Shy', 'å§”å±ˆ': 'Wronged', 'å‘†': 'Blank',
    'ç–‘é—®': 'Question', 'æ™•': 'Dizzy', 'è¡°': 'Depressed', 'å¹æ°”': 'Sigh',
    
    # å¾®ä¿¡å¸¸ç”¨è¡¨æƒ…
    'å¼º': 'Strong', 'å¼±': 'Weak', 'ok': 'OK', 'æ‹œæ‰˜': 'Please',
    'å‘²ç‰™': 'Grin', 'å·ç¬‘': 'Smirk', 'å†è§': 'Goodbye', 'æŠ“ç‹‚': 'Crazy',
    'é¼“æŒ': 'Clap', 'æµæ±—': 'Sweat', 'æ†¨ç¬‘': 'Silly Smile', 'æ‚ é—²': 'Relaxed',
    'å¥‹æ–—': 'Struggle', 'å’’éª‚': 'Curse', 'ç–‘é—®': 'Question', 'å˜˜': 'Shh',
    'æ™•': 'Dizzy', 'æŠ˜ç£¨': 'Torture', 'è¡°': 'Depressed', 'éª·é«…': 'Skull',
    'æ•²æ‰“': 'Punch', 'å†è§': 'Goodbye', 'é—­å˜´': 'Zip it', 'é„™è§†': 'Despise',
    'é—ªç”µ': 'Lightning', 'å‘æŠ–': 'Shiver', 'éš¾è¿‡': 'Sad', 'é…·': 'Cool',
    'æŠ é¼»': 'Nose', 'åç¬‘': 'Evil Smile', 'å·¦å“¼å“¼': 'Hmph Left', 'å³å“¼å“¼': 'Hmph Right',
    'å“ˆæ¬ ': 'Yawn', 'é„™è§†': 'Despise', 'å§”å±ˆ': 'Wronged', 'å¿«å“­äº†': 'About to Cry',
    'é˜´é™©': 'Sinister', 'äº²äº²': 'Kiss', 'å“': 'Scared', 'å¯æ€œ': 'Pitiful',
    'èœåˆ€': 'Knife', 'è¥¿ç“œ': 'Watermelon', 'å•¤é…’': 'Beer', 'ç¯®çƒ': 'Basketball',
    'ä¹’ä¹“': 'Ping Pong', 'æ‹¥æŠ±': 'Hug', 'æ¡æ‰‹': 'Handshake'
}

def contains(text, checklist):
    """
    Check if text contains any pattern in checklist
    
    Args:
        text: Text to check
        checklist: List of regex patterns
        
    Returns:
        bool: True if text contains any pattern
    """
    res = [bool(re.findall(each, text)) for each in checklist]
    return any(res)

def process_chat_data(file_path, analysis_id):
    """
    Process WeChat chat data from CSV file
    
    Args:
        file_path: Path to CSV file
        analysis_id: Unique ID for this analysis
        
    Returns:
        dict: Analysis results
    """
    # Create output directories
    output_dir = os.path.join('static', 'images', analysis_id)
    os.makedirs(output_dir, exist_ok=True)
    
    # Load data
    records = pd.read_csv(file_path, usecols=['IsSender', 'StrContent', 'StrTime']).dropna(how='any')
    
    # Filter out unwanted messages
    not_want_msg = [
        '<.+',  # e.g., <msg, <?xml
        r'^\d{1,}$'  # Pure numbers like verification codes
    ]
    records_not_want = records['StrContent'].apply(lambda x: contains(x, not_want_msg))
    records = records[~records_not_want]
    records.index = range(records.shape[0])
    
    # ä¿®å¤ï¼šæ”¹è¿›åœç”¨è¯æ–‡ä»¶è·¯å¾„çš„è·å–æ–¹å¼
    # ç»å¯¹è·¯å¾„
    current_file_path = os.path.abspath(__file__)
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
    
    emoji_file = os.path.join(base_dir, 'input_data', 'emoji.txt')
    stopword_file = os.path.join(base_dir, 'input_data', 'stopwords_hit_modified.txt')
    transform_file = os.path.join(base_dir, 'input_data', 'transformDict.txt')
    user_dict_file = os.path.join(base_dir, 'input_data', 'userDict.txt')
    
    print(f"Using stopword file: {stopword_file}")
    
    # Load emoji dictionary
    try:
        emoji_eng2cn = pd.read_table(emoji_file).set_index('eng').to_dict()['cn']
        print(f"Successfully loaded emoji dictionary with {len(emoji_eng2cn)} entries")
    except Exception as e:
        print(f"Error loading emoji file: {e}")
        emoji_eng2cn = {}
    
    # ä¿®å¤ï¼šæ”¹è¿›åœç”¨è¯åŠ è½½æ–¹å¼å¹¶æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
    try:
        with open(stopword_file, 'r', encoding='utf-8') as f1:
            stop_words = set(f1.read().splitlines())
        print(f"Successfully loaded {len(stop_words)} stopwords")
        print(f"Sample stopwords: {list(stop_words)[:20]}")
    except Exception as e:
        print(f"Error loading stopwords: {e}")
        # æ·»åŠ ä¸€ä¸ªåŸºæœ¬çš„åœç”¨è¯åˆ—è¡¨ä½œä¸ºå¤‡ç”¨
        stop_words = {'ä¸€ä¸ª','å§','å—','æˆ‘', 'ä½ ', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'ä¸', 'è¿™', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'é‚£', 'å°±', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'åˆ°', 'è¯´'}
        print(f"Using fallback stopwords: {stop_words}")
    
    # Load transform dictionary
    try:
        transformDict = pd.read_table(transform_file).set_index('original').to_dict()['transformed']
    except Exception as e:
        print(f"Error loading transform dictionary: {e}")
        transformDict = {}
    
    # Load user dictionary
    try:
        jieba.load_userdict(user_dict_file)
    except Exception as e:
        print(f"Error loading user dictionary: {e}")
    
    # Process records
    records['keywords'] = [''] * records.shape[0]
    records['emoji'] = [''] * records.shape[0]
    
    # æ–°å¢ï¼šåˆ›å»ºemojiç¬¦å·åˆ—
    if USE_EMOJI_SYMBOLS:
        records['emoji_symbols'] = [''] * records.shape[0]
    
    # Parse dates
    records['StrTime'] = pd.to_datetime(records['StrTime'])
    
    # ä¿®å¤ï¼šæ”¹è¿›å…³é”®è¯æå–å’Œåœç”¨è¯è¿‡æ»¤é€»è¾‘
    for i in range(records.shape[0]):
        result = []
        emoji_res = []
        emoji_symbols = []  # æ–°å¢ï¼šå­˜å‚¨emojiç¬¦å·
        
        # æ‰“å°å‰å‡ æ¡æ¶ˆæ¯å†…å®¹ç”¨äºè°ƒè¯•
        if i < 3:
            print(f"Processing message {i}: {records.loc[i, 'StrContent'][:30]}...")
        
        # åˆ†è¯
        content = records.loc[i, 'StrContent']
        words = list(jieba.cut(content))
        
        if i < 3:
            print(f"Segmented words for message {i} (first 10): {words[:10]}")
        
        # æå–å…³é”®è¯ï¼Œå¼ºåŒ–åœç”¨è¯è¿‡æ»¤
        for word in words:
            word = word.strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åœç”¨è¯
            if word in stop_words:
                if i < 3:
                    print(f"Filtered out stopword: '{word}'")
                continue  # è·³è¿‡åœç”¨è¯
            
            # å…¶ä»–è¿‡æ»¤æ¡ä»¶
            if word and re.findall(r'[\[\]ä¸€-é¾Ÿa-zA-Z0-9]+', word) and \
               not re.findall(r'^\d{1,}$|^\d{1,}\.\d{1,}$', word) and \
               not re.findall('^[a-zA-Z]$', word):
                if word in transformDict:
                    word = transformDict[word]
                result.append(word)
        
        # å¤„ç†è¡¨æƒ…
        for _ in range(result.count(']')):
            ind2 = result.index(']')
            if len(result) < 3:
                break
            if result[ind2 - 2] != '[':
                continue
            emoji_text = result[ind2 - 1]
            
            # ä¿æŒåŸæœ‰çš„è¡¨æƒ…å¤„ç†é€»è¾‘
            if emoji_text in emoji_eng2cn.keys():
                # If English and in dictionary, convert to Chinese
                cur_emoji = '[' + emoji_eng2cn[emoji_text] + ']'
                emoji_text_clean = emoji_eng2cn[emoji_text]  # è½¬æ¢ä¸ºä¸­æ–‡åç§°ä¾›ç¬¦å·æŸ¥æ‰¾
            else:
                if re.findall('^[0-9a-zA-Z]+$', emoji_text):
                    # If emoji_text is all alphanumeric and not in emoji dictionary, delete and skip
                    del result[ind2 - 2:ind2 + 1]
                    continue
                cur_emoji = '[' + emoji_text + ']'  # çº¯æ±‰å­—
                emoji_text_clean = emoji_text  # ç›´æ¥ä½¿ç”¨ä¸­æ–‡åç§°
            
            # æ–°å¢ï¼šè·å–å¯¹åº”çš„emojiç¬¦å·
            if USE_EMOJI_SYMBOLS:
                # å¤„ç†è¡¨æƒ…æ–‡æœ¬ï¼Œå»æ‰å¯èƒ½çš„æ‹¬å·
                if emoji_text_clean.startswith('[') and emoji_text_clean.endswith(']'):
                    emoji_text_clean = emoji_text_clean[1:-1]
                
                # æŸ¥æ‰¾å¯¹åº”çš„emojiç¬¦å·
                emoji_symbol = EMOJI_TO_SYMBOL.get(emoji_text_clean, '')
                
                # å¦‚æœåœ¨å­—å…¸ä¸­æ‰¾ä¸åˆ°å®Œæ•´åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
                if not emoji_symbol:
                    for key, symbol in EMOJI_TO_SYMBOL.items():
                        if key in emoji_text_clean or emoji_text_clean in key:
                            emoji_symbol = symbol
                            break
                
                if emoji_symbol:
                    emoji_symbols.append(emoji_symbol)
                    # è°ƒè¯•ä¿¡æ¯
                    if i < 3:
                        print(f"Found emoji: {emoji_text_clean} -> {emoji_symbol}")
                else:
                    if i < 3:
                        print(f"Could not find emoji symbol for: {emoji_text_clean}")
            
            del result[ind2 - 2:ind2 + 1]
            emoji_res.append(cur_emoji)
        
        # ä¿å­˜å…³é”®è¯å’Œè¡¨æƒ…
        records.at[i, 'keywords'] = ', '.join(result)
        records.at[i, 'emoji'] = ', '.join(emoji_res)
        
        # æ–°å¢ï¼šä¿å­˜emojiç¬¦å·
        if USE_EMOJI_SYMBOLS:
            records.at[i, 'emoji_symbols'] = ', '.join(emoji_symbols)
    
    # Replace empty strings with NaN for easier filtering
    records.replace('', float('nan'), inplace=True)
    
    # Remove records with no keywords or emojis
    records.dropna(how='all', subset=['keywords', 'emoji'], inplace=True)
    
    # Generate analysis results
    analysis_results = {}
    
    # Word frequency analysis
    analysis_results['word_cloud'] = generate_word_cloud(records, output_dir)
    
    # Time analysis
    analysis_results['time_analysis'] = analyze_time(records, output_dir)
    
    # Calendar heatmap
    analysis_results['calendar_heatmap'] = generate_calendar_heatmap(records, output_dir)
    
    # ä¿®å¤ï¼šæ›¿æ¢translate_keywordså‡½æ•°ï¼Œé¿å…å¼‚æ­¥é—®é¢˜
    analysis_results['keywords'] = extract_keywords(records)
    
    # æ–°å¢ï¼šå¦‚æœå¯ç”¨äº†emojiç¬¦å·ï¼Œç”Ÿæˆemojiè¯äº‘
    if USE_EMOJI_SYMBOLS and 'emoji_symbols' in records.columns:
        emoji_cloud_data = generate_emoji_word_cloud(records, output_dir)
        if emoji_cloud_data:
            analysis_results['emoji_symbol_cloud'] = emoji_cloud_data
    
    return analysis_results

def generate_word_cloud(records, output_dir):
    """
    Generate word cloud images
    
    Args:
        records: DataFrame with chat records
        output_dir: Directory to save images
        
    Returns:
        dict: Paths to word cloud images
    """
    # Extract keywords
    all_keywords = ', '.join(records['keywords'].dropna().tolist()).split(', ')
    sender_keywords = ', '.join(records.loc[records['IsSender'] == 1, 'keywords'].dropna().tolist()).split(', ')
    receiver_keywords = ', '.join(records.loc[records['IsSender'] == 0, 'keywords'].dropna().tolist()).split(', ')
    
    # Count word frequencies
    all_counter = Counter(all_keywords)
    sender_counter = Counter(sender_keywords)
    receiver_counter = Counter(receiver_keywords)
    
    # Remove brackets if they exist
    if '[' in all_counter:
        all_counter.pop('[')
    if ']' in all_counter:
        all_counter.pop(']')
    
    # Generate word clouds
    word_cloud_paths = {}
    
    # Both participants
    both_wc_path = os.path.join(output_dir, 'both_wordcloud.html')
    generate_pyecharts_wordcloud(all_counter, 'Both Participants', both_wc_path)
    word_cloud_paths['both'] = both_wc_path
    
    # Sender
    sender_wc_path = os.path.join(output_dir, 'sender_wordcloud.html')
    generate_pyecharts_wordcloud(sender_counter, 'You', sender_wc_path)
    word_cloud_paths['sender'] = sender_wc_path
    
    # Receiver
    receiver_wc_path = os.path.join(output_dir, 'receiver_wordcloud.html')
    generate_pyecharts_wordcloud(receiver_counter, 'Other Person', receiver_wc_path)
    word_cloud_paths['receiver'] = receiver_wc_path
    
    # Generate French version
    # Translate top 100 words
    top_words = [word for word, _ in all_counter.most_common(100)]
    french_translations = {}
    
    try:
        for word in top_words:
            french_translations[word] = translator.translate(word, src='zh-cn', dest='fr').text
        
        # Create French word cloud
        french_counter = Counter({french_translations.get(word, word): count 
                                 for word, count in all_counter.most_common(100)})
        
        french_wc_path = os.path.join(output_dir, 'french_wordcloud.html')
        generate_pyecharts_wordcloud(french_counter, 'Nuage de mots (FranÃ§ais)', french_wc_path)
        word_cloud_paths['french'] = french_wc_path
    except:
        # If translation fails, skip French word cloud
        pass
    
    return word_cloud_paths

def generate_pyecharts_wordcloud(word_counter, title, output_path):
    """
    Generate word cloud using pyecharts
    
    Args:
        word_counter: Counter object with word frequencies
        title: Title for the word cloud
        output_path: Path to save the HTML file
    """
    word_pairs = [(word, count) for word, count in word_counter.most_common(150)]
    
    wc = (
        PyechartsWordCloud(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))
        .add("", word_pairs, word_size_range=[15, 80], shape="circle")
        .set_global_opts(
            title_opts=opts.TitleOpts(
                title=title,
                title_textstyle_opts=opts.TextStyleOpts(font_size=25, color="midnightblue")
            )
        )
    )
    
    wc.render(output_path)

def analyze_time(records, output_dir):
    """
    Analyze message frequency by time
    """
    # Calculate time span
    date0 = records['StrTime'].min().date()
    date1 = records['StrTime'].max().date()
    n_date = (date1 - date0).days + 1
    
    # Add year-month and hour columns with format that ensures proper chronological sorting
    records['year-month'] = records['StrTime'].apply(lambda x: f"{x.year}-{str(x.month).zfill(2)}")
    records['hour'] = records['StrTime'].apply(lambda x: str(x.hour).zfill(2))
    
    # Monthly data
    monthly_data = records.groupby(by='year-month')['year-month'].count()
    
    # Hourly data
    hourly_data = records.groupby(by='hour')['hour'].count() / n_date
    
    # Fill missing hours with zeros
    missing_hours = set(str(each).zfill(2) for each in range(24)) - set(hourly_data.index)
    zero_hours = pd.Series([0 for _ in range(len(missing_hours))], index=missing_hours)
    hourly_data = pd.concat((hourly_data, zero_hours))
    hourly_data.sort_index(inplace=True)
    
    # Generate monthly chart
    plt.figure(figsize=(12, 8))
    plt.rc('font', family='SimSun', size=15)
    plt.rcParams['axes.unicode_minus'] = False
    
    # Use consistent color for all bars
    plt.bar(monthly_data.index, monthly_data.values, color='#1f77b4')  # Standard blue color
    
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Message Count', fontsize=15)
    plt.title('Monthly Message Count', fontsize=20)
    
    monthly_path = os.path.join(output_dir, 'monthly_messages.png')
    plt.tight_layout()  # Improve layout
    plt.savefig(monthly_path)
    plt.close()
    
    # Generate hourly chart
    plt.figure(figsize=(12, 8))
    plt.rc('font', family='SimSun', size=15)
    
    x2 = list(range(24))
    plt.bar(x2, hourly_data, color='#1f77b4')  # Standard blue color
    plt.xticks(x2, hourly_data.index)
    plt.xlabel('Hour', fontsize=20)
    plt.ylabel('Average Message Count', fontsize=20)
    plt.title('Hourly Message Distribution', fontsize=20)
    
    hourly_path = os.path.join(output_dir, 'hourly_messages.png')
    plt.tight_layout()  # Improve layout
    plt.savefig(hourly_path)
    plt.close()
    
    return {
        'first_date': date0.strftime('%Y-%m-%d'),
        'last_date': date1.strftime('%Y-%m-%d'),
        'total_days': n_date,
        'total_messages': len(records),
        'monthly_chart': monthly_path,
        'hourly_chart': hourly_path
    }

def generate_calendar_heatmap(records, output_dir):
    """
    Generate calendar heatmap showing message frequency by date
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert to DataFrame if it's a list
    if isinstance(records, list):
        df = pd.DataFrame(records)
    else:
        df = records
    
    # Count messages by date
    df['date'] = pd.to_datetime(df['StrTime']).dt.date
    date_counts = df.groupby('date').size().reset_index(name='count')
    
    # Get date range
    min_date = date_counts['date'].min()
    max_date = date_counts['date'].max()
    
    # Only use actual data range
    date_range = pd.date_range(start=min_date, end=max_date)
    
    # Create date index
    date_index = pd.DataFrame({'date': date_range})
    date_index['date'] = date_index['date'].dt.date
    
    # Merge with counts
    merged_df = date_index.merge(date_counts, on='date', how='left').fillna(0)
    
    # Format for heatmap with year-month for proper chronological sorting
    merged_df['year_month'] = pd.to_datetime(merged_df['date']).dt.strftime('%Y-%m')
    merged_df['day'] = pd.to_datetime(merged_df['date']).dt.day
    
    # Create pivot table
    pivot_df = merged_df.pivot(index='day', columns='year_month', values='count')
    
    # Reorder columns to ensure chronological order
    pivot_df = pivot_df[sorted(pivot_df.columns)]
    
    # Set up plot with better aspect ratio
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Set font for characters
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    
    # Create heatmap with improved color scheme
    cmap = plt.cm.Blues
    heatmap = ax.pcolor(pivot_df, cmap=cmap)
    
    # Format axes
    ax.set_yticks(np.arange(pivot_df.shape[0]) + 0.5)
    ax.set_yticklabels(pivot_df.index)
    ax.set_xticks(np.arange(pivot_df.shape[1]) + 0.5)
    ax.set_xticklabels(pivot_df.columns)
    
    # Add colorbar with better positioning
    cbar = plt.colorbar(heatmap, fraction=0.046, pad=0.04)
    cbar.set_label('Message Count', rotation=270, labelpad=15)
    
    # Title with year range
    min_year = min_date.year
    max_year = max_date.year
    year_range = f"{min_year}" if min_year == max_year else f"{min_year}-{max_year}"
    plt.title(f"Chat Frequency Heatmap {year_range}")
    
    # Save image
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    output_path = os.path.join(output_dir, f'calendar_heatmap_{timestamp}.png')
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return output_path

def extract_keywords(records):
    """
    Extract keywords without translation for sentiment analysis
    """
    # Extract all keywords
    all_keywords = ', '.join(records['keywords'].dropna().tolist()).split(', ')
    
    # Count word frequencies
    word_counter = Counter(all_keywords)
    
    # Get top 200 words
    top_words = [word for word, _ in word_counter.most_common(200)]
    
    return {
        'original': top_words,
        'translated': top_words,  # Same as original, no translation needed
        'frequencies': [word_counter[word] for word in top_words]
    }

def generate_emoji_word_cloud(records, output_dir):
    """
    ç”Ÿæˆæ”¯æŒemojiçš„è¯äº‘
    
    Args:
        records: DataFrame with chat records
        output_dir: Directory to save images
        
    Returns:
        dict: Paths to emoji word cloud images
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰emojiç¬¦å·æ•°æ®
    if 'emoji_symbols' not in records.columns or records['emoji_symbols'].dropna().empty:
        print("No emoji symbols found, skipping emoji word cloud generation")
        return None
    
    # æå–æ‰€æœ‰emojiç¬¦å·
    all_emojis = []
    sender_emojis = []
    receiver_emojis = []
    
    # é€è¡Œå¤„ç†emojiç¬¦å·ï¼Œé¿å…ä¸¢å¤±æ•°æ®
    for _, row in records.iterrows():
        if pd.notna(row.get('emoji_symbols')) and row['emoji_symbols']:
            emojis = row['emoji_symbols'].split(', ')
            all_emojis.extend([e for e in emojis if e])
            
            if row['IsSender'] == 1:
                sender_emojis.extend([e for e in emojis if e])
            else:
                receiver_emojis.extend([e for e in emojis if e])
    
    if not all_emojis:
        print("No valid emoji symbols found after filtering")
        return None
    
    # è®¡æ•°
    all_counter = Counter(all_emojis)
    sender_counter = Counter(sender_emojis)
    receiver_counter = Counter(receiver_emojis)
    
    # ç§»é™¤ç©ºå€¼
    if '' in all_counter:
        all_counter.pop('')
    
    print(f"Found {len(all_counter)} unique emoji symbols")
    print(f"Most common emojis: {all_counter.most_common(5)}")
    
    # ç”Ÿæˆè¯äº‘
    emoji_cloud_paths = {}
    
    # å‘é€è€…
    if sender_counter:
        print(f"Sender emojis: {sender_counter.most_common(5)}")
        sender_emoji_path = os.path.join(output_dir, 'sender_emoji_cloud.html')
        if generate_emoji_pyecharts_wordcloud(sender_counter, 'Your Emoji Usage', sender_emoji_path):
            emoji_cloud_paths['sender'] = sender_emoji_path
    
    # æ¥æ”¶è€…
    if receiver_counter:
        print(f"Receiver emojis: {receiver_counter.most_common(5)}")
        receiver_emoji_path = os.path.join(output_dir, 'receiver_emoji_cloud.html')
        if generate_emoji_pyecharts_wordcloud(receiver_counter, 'Other Person Emoji Usage', receiver_emoji_path):
            emoji_cloud_paths['receiver'] = receiver_emoji_path
    
    return emoji_cloud_paths

def generate_emoji_pyecharts_wordcloud(emoji_counter, title, output_path):
    """
    ç”Ÿæˆæ”¯æŒemojiçš„è¯äº‘
    
    Args:
        emoji_counter: Counter object with emoji frequencies
        title: Title for the word cloud
        output_path: Path to save the HTML file
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not emoji_counter:
        return False
    
    # å‡†å¤‡å¸¦æœ‰tooltipçš„è¯äº‘æ•°æ®
    word_pairs = []
    tooltip_data = []
    
    # ä¸è®¾ç½®ä¸Šé™ï¼Œæ˜¾ç¤ºæ‰€æœ‰emoji
    for emoji, count in emoji_counter.most_common():
        if emoji and count > 0:
            # æŸ¥æ‰¾å¯¹åº”çš„ä¸­æ–‡è¡¨æƒ…åç§°å’Œè‹±æ–‡ç¿»è¯‘
            found = False
            for ch_name, symbol in EMOJI_TO_SYMBOL.items():
                if symbol == emoji:
                    english_name = EMOJI_TO_ENGLISH.get(ch_name, ch_name)
                    word_pairs.append((emoji, count))
                    tooltip_data.append(f"{english_name} ({count})")
                    found = True
                    break
            
            if not found:
                # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„ä¸­æ–‡åç§°ï¼Œç›´æ¥ä½¿ç”¨emoji
                word_pairs.append((emoji, count))
                tooltip_data.append(f"Emoji ({count})")
    
    if not word_pairs:
        print("No valid emoji word pairs found")
        return False
    
    print(f"Generating emoji word cloud with {len(word_pairs)} emojis")
    
    try:
        # åˆ›å»ºè¯äº‘ï¼Œä¸ä½¿ç”¨å†…ç½®tooltip
        wc = (
            PyechartsWordCloud(init_opts=opts.InitOpts(
                theme=ThemeType.LIGHT,
                width="100%",
                height="400px"  # å¢åŠ é«˜åº¦ä»¥å®¹çº³æ›´å¤šemoji
            ))
            .add("", 
                 word_pairs,  # åªä¼ é€’emojiå’Œé¢‘ç‡
                 word_size_range=[20, 80],  # è°ƒæ•´è¯äº‘å­—ä½“å¤§å°
                 shape="circle", 
                 textstyle_opts=opts.TextStyleOpts(
                     font_family="'Segoe UI Emoji', 'Apple Color Emoji', 'Noto Color Emoji', sans-serif")
                )
            .set_global_opts(
                title_opts=opts.TitleOpts(
                    title=title,
                    title_textstyle_opts=opts.TextStyleOpts(font_size=22, color="midnightblue")
                )
            )
        )
        
        # æ¸²æŸ“è¯äº‘
        wc.render(output_path)
        
        # è¯»å–æ¸²æŸ“åçš„HTML
        with open(output_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # æ·»åŠ è‡ªå®šä¹‰CSSå’ŒJavaScript
        custom_style = """
        <style>
        .chart-container {
            height: 400px !important;
            overflow: visible !important;
        }
        .chart-container text {
            font-family: 'Segoe UI Emoji', 'Apple Color Emoji', 'Noto Color Emoji', sans-serif;
            font-size: 1.5em !important;
            cursor: pointer;
        }
        .chart-container svg {
            overflow: visible !important;
        }
        .tooltip {
            position: absolute;
            display: none;
            background: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            font-family: Arial, sans-serif;
            font-size: 14px;
            z-index: 100;
            pointer-events: none;
        }
        </style>
        <div class="tooltip" id="custom_tooltip"></div>
        <script>
        // å®šä¹‰tooltipæ•°æ®
        var tooltipData = """ + str(tooltip_data).replace("'", '"') + """;
        
        window.addEventListener('load', function() {
            var chart = document.querySelector('.chart-container');
            var tooltip = document.getElementById('custom_tooltip');
            
            // è·å–æ‰€æœ‰emojiæ–‡æœ¬å…ƒç´ 
            var textElements = chart.querySelectorAll('text');
            
            // ä¸ºæ¯ä¸ªæ–‡æœ¬å…ƒç´ æ·»åŠ äº‹ä»¶
            for (var i = 0; i < textElements.length; i++) {
                if (i < tooltipData.length) {
                    let tooltipText = tooltipData[i];
                    textElements[i].addEventListener('mousemove', function(e) {
                        tooltip.textContent = tooltipText;
                        tooltip.style.display = 'block';
                        tooltip.style.left = (e.pageX + 10) + 'px';
                        tooltip.style.top = (e.pageY + 10) + 'px';
                    });
                    
                    textElements[i].addEventListener('mouseout', function() {
                        tooltip.style.display = 'none';
                    });
                }
            }
        });
        </script>
        """
        
        # æ›¿æ¢HTMLå†…å®¹
        html_content = html_content.replace('</body>', custom_style + '</body>')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"Successfully generated emoji word cloud with tooltips: {output_path}")
        return True
    except Exception as e:
        print(f"Error generating emoji word cloud: {e}")
        return False 